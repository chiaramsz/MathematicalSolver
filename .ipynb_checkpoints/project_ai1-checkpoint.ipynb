{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "## Looking at a sample\n",
    "plt.imshow(X_train[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "print (y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-b844b18cbc5a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-b844b18cbc5a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    >We need to shape the pictures in that way, that the models expects it.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the Data\n",
    ">We need to shape the pictures in that way, that the models expects it.\n",
    "The first number is the number of the pictures we have, the second one is the shape of the image, so in our case it is 28. The last number indicates, that it is a greyscale picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of X_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Checking out the shapes involved in dataset\n",
    "print (\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print (\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print (\"Shape of X_test: {}\".format(X_test.shape))\n",
    "print (\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making the correct shape for the model\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-4f3fa87385b1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-4f3fa87385b1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    > we need this, to get the correct number back, which we recognized. So instead of printing out, that it is number 5, we have an array, where each place means a number. When there is a one, than the field is the predicted number. In our case, the array will look like this #[0,0,0,0,0,1,0,0,0,0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###Encoding\n",
    "> we need this, to get the correct number back, which we recognized. So instead of printing out, that it is number 5, we have an array, where each place means a number. When there is a one, than the field is the predicted number. In our case, the array will look like this [0,0,0,0,0,1,0,0,0,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "model = Sequential()\n",
    "\n",
    "## Layers\n",
    "layer_1 = Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1))\n",
    "layer_2 = MaxPooling2D(pool_size=2)\n",
    "layer_3 = Conv2D(32, kernel_size=3, activation='relu')\n",
    "layer_4 = MaxPooling2D(pool_size=2)\n",
    "layer_5 = Dropout(0.5)\n",
    "layer_6 = Flatten()\n",
    "layer_7 = Dense(128, activation=\"relu\")\n",
    "layer_8 = Dropout(0.5)\n",
    "layer_9 = Dense(10, activation='softmax')\n",
    "\n",
    "## Adding the layers to the model\n",
    "model.add(layer_1)\n",
    "model.add(layer_2)\n",
    "model.add(layer_3)\n",
    "model.add(layer_4)\n",
    "model.add(layer_5)\n",
    "model.add(layer_6)\n",
    "model.add(layer_7)\n",
    "model.add(layer_8)\n",
    "model.add(layer_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-8e8e45dcb4b6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-8e8e45dcb4b6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    > We will use a Sequential Model, because that is the easiest way to build a model in Keras. It is going to be build layer by layer.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "> We will use a Sequential Model, because that is the easiest way to build a model in Keras. It is going to be build layer by layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.9687 - accuracy: 0.6625 - val_loss: 0.1053 - val_accuracy: 0.9685\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 55s 30ms/step - loss: 0.2853 - accuracy: 0.9151 - val_loss: 0.0906 - val_accuracy: 0.9713\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.2156 - accuracy: 0.9359 - val_loss: 0.0771 - val_accuracy: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5285ca6d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We achieved a great accuracy with over 97%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (Softmax) from the neural network:\n",
      "\n",
      " [[9.9989212e-01 1.6386188e-07 5.5602900e-06 6.3315371e-09 3.3692763e-07\n",
      "  8.8783466e-08 9.4727744e-05 2.8604918e-08 5.3125214e-06 1.6869549e-06]]\n",
      "\n",
      "\n",
      "Hard-maxed form of the prediction: \n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "--------- Prediction --------- \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOF0lEQVR4nO3dcYxV5ZnH8d8jLUalENQsTkTXboN/NI0OgoSkZqU2bSyaQGNSIcah2SZDYkmoaUy1HYVk3dgYZaMmEqdKipUVquiCzVpqGaLbmDSOSBV1W6lBC46MqJEhJrLC0z/uoRlxznuGe8+558Lz/SSTe+955tz7eJmf59zznntec3cBOPmdUncDANqDsANBEHYgCMIOBEHYgSC+0M4XMzMO/QMVc3cba3lLW3Yzu9LM/mxmu8zs5laeC0C1rNlxdjObIOkvkr4laY+kFyQtdvfXEuuwZQcqVsWWfY6kXe7+prsfkrRe0oIWng9AhVoJ+7mS/jbq8Z5s2WeYWa+ZDZrZYAuvBaBFlR+gc/d+Sf0Su/FAnVrZsu+VdN6ox9OzZQA6UCthf0HSDDP7splNlLRI0uZy2gJQtqZ34939UzNbJmmLpAmS1rj7q6V1BqBUTQ+9NfVifGYHKlfJSTUAThyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTR1imbcfKZNWtWsr5s2bLcWk9PT3Ldhx9+OFm/7777kvXt27cn69GwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJjFFUnd3d3J+sDAQLI+efLkErv5rI8++ihZP+ussyp77U6WN4trSyfVmNluSSOSDkv61N1nt/J8AKpTxhl033D3/SU8D4AK8ZkdCKLVsLuk35nZi2bWO9YvmFmvmQ2a2WCLrwWgBa3uxl/m7nvN7J8kPWNm/+fuz43+BXfvl9QvcYAOqFNLW3Z335vdDkt6UtKcMpoCUL6mw25mZ5jZl47el/RtSTvLagxAuVrZjZ8m6UkzO/o8/+Xuvy2lK7TNnDnpnbGNGzcm61OmTEnWU+dxjIyMJNc9dOhQsl40jj537tzcWtF33Yte+0TUdNjd/U1JF5fYC4AKMfQGBEHYgSAIOxAEYQeCIOxAEHzF9SRw+umn59YuueSS5LqPPPJIsj59+vRkPRt6zZX6+yoa/rrzzjuT9fXr1yfrqd76+vqS695xxx3JeifL+4orW3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIpm08CDzzwQG5t8eLFbezk+BSdAzBp0qRk/dlnn03W582bl1u76KKLkuuejNiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLOfAGbNmpWsX3XVVbm1ou+bFykay37qqaeS9bvuuiu39s477yTXfemll5L1Dz/8MFm/4oorcmutvi8nIrbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE143vAN3d3cn6wMBAsj558uSmX/vpp59O1ou+D3/55Zcn66nvjT/44IPJdd97771kvcjhw4dzax9//HFy3aL/rqJr3tep6evGm9kaMxs2s52jlp1pZs+Y2RvZ7dQymwVQvvHsxv9S0pXHLLtZ0lZ3nyFpa/YYQAcrDLu7Pyfpg2MWL5C0Nru/VtLCctsCULZmz42f5u5D2f13JU3L+0Uz65XU2+TrAChJy1+EcXdPHXhz935J/RIH6IA6NTv0ts/MuiQpux0uryUAVWg27JslLcnuL5G0qZx2AFSlcJzdzB6VNE/S2ZL2SVoh6b8l/VrS+ZLekvQ9dz/2IN5YzxVyN/7CCy9M1lesWJGsL1q0KFnfv39/bm1oaCi3Jkm33357sv74448n650sNc5e9He/YcOGZP26665rqqd2yBtnL/zM7u55Z1V8s6WOALQVp8sCQRB2IAjCDgRB2IEgCDsQBJeSLsGpp56arKcupyxJ8+fPT9ZHRkaS9Z6entza4OBgct3TTjstWY/q/PPPr7uF0rFlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGcvwcyZM5P1onH0IgsWLEjWi6ZVBiS27EAYhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsJVi1alWybjbmlX3/oWicnHH05pxySv627MiRI23spDOwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnH6err746t9bd3Z1ct2h64M2bNzfTEgqkxtKL/k127NhRcjf1K9yym9kaMxs2s52jlq00s71mtiP7ae3qDAAqN57d+F9KunKM5f/p7t3Zz/+U2xaAshWG3d2fk/RBG3oBUKFWDtAtM7OXs938qXm/ZGa9ZjZoZulJxwBUqtmwr5b0FUndkoYk3Z33i+7e7+6z3X12k68FoARNhd3d97n7YXc/IukXkuaU2xaAsjUVdjPrGvXwu5J25v0ugM5QOM5uZo9KmifpbDPbI2mFpHlm1i3JJe2WtLS6FjtDah7ziRMnJtcdHh5O1jds2NBUTye7onnvV65c2fRzDwwMJOu33HJL08/dqQrD7u6Lx1j8UAW9AKgQp8sCQRB2IAjCDgRB2IEgCDsQBF9xbYNPPvkkWR8aGmpTJ52laGitr68vWb/pppuS9T179uTW7r4796RPSdLBgweT9RMRW3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9jaIfKno1GW2i8bJr7322mR906ZNyfo111yTrEfDlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcfZzMrKmaJC1cuDBZX758eTMtdYQbb7wxWb/11ltza1OmTEmuu27dumS9p6cnWcdnsWUHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZx8nd2+qJknnnHNOsn7vvfcm62vWrEnW33///dza3Llzk+tef/31yfrFF1+crE+fPj1Zf/vtt3NrW7ZsSa57//33J+s4PoVbdjM7z8y2mdlrZvaqmS3Plp9pZs+Y2RvZ7dTq2wXQrPHsxn8q6cfu/lVJcyX90My+KulmSVvdfYakrdljAB2qMOzuPuTu27P7I5Jel3SupAWS1ma/tlbSwop6BFCC4/rMbmYXSJop6Y+Sprn70UnK3pU0LWedXkm9LfQIoATjPhpvZpMkbZT0I3c/MLrmjSNUYx6lcvd+d5/t7rNb6hRAS8YVdjP7ohpBX+fuT2SL95lZV1bvkjRcTYsAylC4G2+N728+JOl1d181qrRZ0hJJP89u09f1DWzChAnJ+g033JCsF10S+cCBA7m1GTNmJNdt1fPPP5+sb9u2Lbd22223ld0OEsbzmf3rkq6X9IqZ7ciW/VSNkP/azH4g6S1J36ukQwClKAy7u/9BUt7VGb5ZbjsAqsLpskAQhB0IgrADQRB2IAjCDgRhRV/PLPXFzNr3YiVLfZXzscceS6576aWXtvTaRZeqbuXfMPX1WElav359sn4iXwb7ZOXuY/7BsGUHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZy9BV1dXsr506dJkva+vL1lvZZz9nnvuSa67evXqZH3Xrl3JOjoP4+xAcIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7MBJhnF2IDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiiMOxmdp6ZbTOz18zsVTNbni1faWZ7zWxH9jO/+nYBNKvwpBoz65LU5e7bzexLkl6UtFCN+dgPuvtd434xTqoBKpd3Us145mcfkjSU3R8xs9clnVtuewCqdlyf2c3sAkkzJf0xW7TMzF42szVmNjVnnV4zGzSzwdZaBdCKcZ8bb2aTJD0r6T/c/QkzmyZpvySX9O9q7Or/W8FzsBsPVCxvN35cYTezL0r6jaQt7r5qjPoFkn7j7l8reB7CDlSs6S/CWOPSpg9Jen100LMDd0d9V9LOVpsEUJ3xHI2/TNL/SnpF0pFs8U8lLZbUrcZu/G5JS7ODeannYssOVKyl3fiyEHagenyfHQiOsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EEThBSdLtl/SW6Men50t60Sd2lun9iXRW7PK7O2f8wpt/T77517cbNDdZ9fWQEKn9tapfUn01qx29cZuPBAEYQeCqDvs/TW/fkqn9tapfUn01qy29FbrZ3YA7VP3lh1AmxB2IIhawm5mV5rZn81sl5ndXEcPecxst5m9kk1DXev8dNkcesNmtnPUsjPN7BkzeyO7HXOOvZp664hpvBPTjNf63tU9/XnbP7Ob2QRJf5H0LUl7JL0gabG7v9bWRnKY2W5Js9299hMwzOxfJR2U9PDRqbXM7E5JH7j7z7P/UU519590SG8rdZzTeFfUW940499Xje9dmdOfN6OOLfscSbvc/U13PyRpvaQFNfTR8dz9OUkfHLN4gaS12f21avyxtF1Obx3B3YfcfXt2f0TS0WnGa33vEn21RR1hP1fS30Y93qPOmu/dJf3OzF40s966mxnDtFHTbL0raVqdzYyhcBrvdjpmmvGOee+amf68VRyg+7zL3P0SSd+R9MNsd7UjeeMzWCeNna6W9BU15gAcknR3nc1k04xvlPQjdz8wulbnezdGX2153+oI+15J5416PD1b1hHcfW92OyzpSTU+dnSSfUdn0M1uh2vu5x/cfZ+7H3b3I5J+oRrfu2ya8Y2S1rn7E9ni2t+7sfpq1/tWR9hfkDTDzL5sZhMlLZK0uYY+PsfMzsgOnMjMzpD0bXXeVNSbJS3J7i+RtKnGXj6jU6bxzptmXDW/d7VPf+7ubf+RNF+NI/J/lfSzOnrI6etfJP0p+3m17t4kParGbt3/q3Fs4weSzpK0VdIbkn4v6cwO6u1Xakzt/bIaweqqqbfL1NhFf1nSjuxnft3vXaKvtrxvnC4LBMEBOiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4u8I826N2+OQkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final Output: 0\n"
     ]
    }
   ],
   "source": [
    "example = X_train[1]\n",
    "prediction = model.predict(example.reshape(1, 28, 28, 1))\n",
    "print (\"Prediction (Softmax) from the neural network:\\n\\n {}\".format(prediction))\n",
    "hard_maxed_prediction = np.zeros(prediction.shape)\n",
    "hard_maxed_prediction[0][np.argmax(prediction)] = 1\n",
    "print (\"\\n\\nHard-maxed form of the prediction: \\n\\n {}\".format(hard_maxed_prediction))\n",
    "\n",
    "print (\"\\n\\n--------- Prediction --------- \\n\\n\")\n",
    "plt.imshow(example.reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"\\n\\nFinal Output: {}\".format(np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing it with a real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5bc35d750b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./testing.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY_INV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_EXTERNAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('./testing.jpg')\n",
    "grey = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, thresh = cv2.threshold(grey.copy(), 75, 255, cv2.THRESH_BINARY_INV)\n",
    "_, contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "preprocessed_digits = []\n",
    "\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    \n",
    "    # Creating a rectangle around the digit in the original image (for displaying the digits fetched via contours)\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "    # Cropping out the digit from the image corresponding to the current contours in the for loop\n",
    "    digit = thresh[y:y+h, x:x+w]\n",
    "    \n",
    "    # Resizing that digit to (18, 18)\n",
    "    resized_digit = cv2.resize(digit, (18,18))\n",
    "    \n",
    "    # Padding the digit with 5 pixels of black color (zeros) in each side to finally produce the image of (28, 28)\n",
    "    padded_digit = np.pad(resized_digit, ((5,5),(5,5)), \"constant\", constant_values=0)\n",
    "    \n",
    "    # Adding the preprocessed digit to the list of preprocessed digits\n",
    "    preprocessed_digits.append(padded_digit)\n",
    "\n",
    "print(\"\\n\\n\\n----------------Contoured Image--------------------\")\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "    \n",
    "inp = np.array(preprocessed_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
